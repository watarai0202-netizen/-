from __future__ import annotations

import hashlib
import re
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, Optional, Tuple

import requests
import streamlit as st

from src.tdnet import fetch_tdnet_items
from src.analyzer import analyze_pdf_to_json, ai_is_enabled
from src.storage import init_db, get_cached_analysis, save_analysis, db_path_default
from src.viz import render_analysis

# ----------------------------
# Helpers
# ----------------------------

# æ±ºç®—ã£ã½ã„ã‚¿ã‚¤ãƒˆãƒ«åˆ¤å®šï¼ˆã‚†ã‚‹ã‚ï¼‰
_KESSAN_RE = re.compile(
    r"(æ±ºç®—çŸ­ä¿¡|å››åŠæœŸæ±ºç®—|é€šæœŸæ±ºç®—|Financial Results|Earnings|Results)",
    re.IGNORECASE,
)


def is_kessan(title: str) -> bool:
    return bool(_KESSAN_RE.search(title or ""))


def _parse_dt_any(value: Any) -> Optional[datetime]:
    """
    published_at ã®æºã‚Œã«è€ãˆã‚‹ï¼š
      - ISO: 2026-02-06T20:00:00Z / +09:00
      - ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Š: 2026-02-06 20:00:00  (â†JSTæƒ³å®š)
    è¿”ã‚Šå€¤ã¯UTC tz-aware datetime
    """
    if not value:
        return None
    s = str(value).strip()
    if not s:
        return None

    # ISO Z å¯¾å¿œ
    s_iso = s.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(s_iso)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone(timedelta(hours=9)))  # tzç„¡ã—ã¯JSTæƒ³å®š
        return dt.astimezone(timezone.utc)
    except Exception:
        pass

    # "YYYY-MM-DD HH:MM:SS" / "YYYY/MM/DD HH:MM:SS"
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
        try:
            dt = datetime.strptime(s, fmt).replace(tzinfo=timezone(timedelta(hours=9)))
            return dt.astimezone(timezone.utc)
        except Exception:
            continue

    return None


def _extract_tdnet_fields(it: Dict[str, Any]) -> Tuple[str, str, str, Optional[datetime]]:
    """
    it ã®æ­£è¦åŒ–ãŒå£Šã‚Œã¦ã‚‚ app å´ã§å¾©å…ƒã™ã‚‹ï¼ˆå£Šã‚Œã¥ã‚‰ã•å„ªå…ˆï¼‰ã€‚
    æˆ»ã‚Š: (title, code, doc_url, published_at_utc)
    """
    title = (it.get("title") or "").strip()
    code = str(it.get("code") or "").strip()
    doc_url = (it.get("doc_url") or "").strip()
    published_at = it.get("published_at")

    if not isinstance(published_at, datetime):
        published_at = _parse_dt_any(published_at)

    # raw ã‹ã‚‰æ•‘æ¸ˆï¼ˆit["raw"] ã®ä¸‹ãŒ Tdnet/TDnet/ç›´ä¸‹ ãªã©æºã‚Œã‚‹ï¼‰
    raw = it.get("raw") if isinstance(it.get("raw"), dict) else {}
    td: Any = None
    if isinstance(raw.get("Tdnet"), dict):
        td = raw["Tdnet"]
    elif isinstance(raw.get("TDnet"), dict):
        td = raw["TDnet"]
    elif isinstance(raw.get("tdnet"), dict):
        td = raw["tdnet"]
    elif isinstance(raw, dict):
        td = raw

    if isinstance(td, dict):
        if not title:
            title = str(td.get("title") or td.get("Title") or "").strip()

        # 4æ¡/5æ¡æºã‚Œï¼šcompany_code ãŒ 45230 ã¿ãŸã„ã«æœ«å°¾0ã®ã“ã¨ãŒã‚ã‚‹
        if not code:
            code = str(td.get("code") or td.get("company_code") or td.get("Code") or "").strip()

        if not doc_url:
            doc_url = str(
                td.get("document_url")
                or td.get("documentUrl")
                or td.get("doc_url")
                or td.get("url")
                or ""
            ).strip()

        if published_at is None:
            published_at = _parse_dt_any(td.get("published_at") or td.get("pubdate") or td.get("date"))

    return title, code, doc_url, published_at


def _code4(code: str) -> str:
    """
    45230 -> 4523 ã¿ãŸã„ãªæ•‘æ¸ˆï¼ˆæœ«å°¾0ãŒä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³ç”¨ï¼‰ã€‚
    ãŸã ã—å¿…ãšãã†ã¨ã¯é™ã‚‰ãªã„ã®ã§ã€è¡¨ç¤ºã¯ (å…ƒã‚³ãƒ¼ãƒ‰) ã‚‚ä½µè¨˜ã™ã‚‹ã€‚
    """
    c = (code or "").strip()
    if len(c) == 5 and c.isdigit() and c.endswith("0"):
        return c[:-1]
    if len(c) >= 4 and c[:4].isdigit():
        return c[:4]
    return c


def _is_allowed_pdf_url(url: str) -> bool:
    """
    æ‰‹å‹•URLè§£æã®å®‰å…¨ç­–ï¼ˆå£Šã‚Œé˜²æ­¢ï¼‰ã€‚
    - release.tdnet.info ã®PDF
    - yanoshin rd.php çµŒç”±ã§ release.tdnet.info ã®PDF
    """
    u = (url or "").strip()
    if not u:
        return False
    u_low = u.lower()
    if "release.tdnet.info" in u_low and u_low.endswith(".pdf"):
        return True
    if "webapi.yanoshin.jp/rd.php?" in u_low and "release.tdnet.info" in u_low and ".pdf" in u_low:
        return True
    return False


def _pdf_size_bytes(url: str, timeout: float = 10.0) -> Optional[int]:
    """
    HEAD ã§ Content-Length ã‚’å–å¾—ã—ã¦PDFã‚µã‚¤ã‚ºæ¨å®šã€‚
    å–ã‚Œãªã„/å¤±æ•—ã¯ Noneã€‚
    """
    try:
        r = requests.head(url, allow_redirects=True, timeout=timeout)
        if r.status_code >= 400:
            return None
        cl = r.headers.get("Content-Length")
        if not cl:
            return None
        n = int(cl)
        return n if n > 0 else None
    except Exception:
        return None


def _check_pdf_size_or_warn(url: str, max_bytes: int) -> bool:
    """
    max_bytes>0 ã®ã¨ãã€åˆ¤æ˜ã™ã‚‹ç¯„å›²ã§ã‚µã‚¤ã‚ºä¸Šé™è¶…ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã€‚
    ä¸æ˜ãªã‚‰è­¦å‘Šã—ã¦é€šã™ï¼ˆæœ€çµ‚é˜²å¾¡ã¯ analyzer å´æ¨å¥¨ï¼‰ã€‚
    """
    if max_bytes <= 0:
        return True
    n = _pdf_size_bytes(url)
    if n is None:
        st.warning("PDFã‚µã‚¤ã‚ºï¼ˆContent-Lengthï¼‰ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä¸Šé™è¶…ã®å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯è§£æã«å¤±æ•—ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚")
        return True
    if n > max_bytes:
        st.error(f"PDFãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã¾ã™ï¼š{n/1024/1024:.1f}MB > {max_bytes/1024/1024:.1f}MB")
        return False
    return True


def _safe_pdf_link(doc_url: str) -> None:
    """
    link_button ã¯ key éå¯¾å¿œã‚„å¼•æ•°ä»•æ§˜å¤‰æ›´ã§è½ã¡ã‚„ã™ã„ã®ã§ã€
    Markdownãƒªãƒ³ã‚¯ã§å®‰å®šè¡¨ç¤ºã™ã‚‹ã€‚
    """
    u = (doc_url or "").strip()
    if u.startswith("http"):
        st.markdown(f"[PDFã‚’é–‹ã]({u})")
        st.caption(f"PDF: {u}")
    elif u:
        st.warning("PDF URLãŒä¸æ­£å½¢å¼ã®ãŸã‚ãƒªãƒ³ã‚¯ã‚’å‡ºã›ã¾ã›ã‚“ã€‚")
        st.code(u)
    else:
        st.caption("PDF: ï¼ˆãªã—ï¼‰")


# ----------------------------
# Page
# ----------------------------
st.set_page_config(page_title="æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒŠãƒ¼", layout="wide")

# ----------------------------
# Auth (simple password gate)
# ----------------------------
APP_PASSWORD = st.secrets.get("APP_PASSWORD", "")
if not APP_PASSWORD:
    st.error("APP_PASSWORD ãŒæœªè¨­å®šã§ã™ï¼ˆStreamlit Cloud ã® Secrets ã‹ã€ãƒ­ãƒ¼ã‚«ãƒ«ã® .streamlit/secrets.toml ã«è¨­å®šã—ã¦ãã ã•ã„ï¼‰")
    st.stop()

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.title("èªè¨¼ãŒå¿…è¦ã§ã™")
    pw = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰", type="password")
    if pw and pw == APP_PASSWORD:
        st.session_state.authenticated = True
        st.rerun()
    st.stop()

# ----------------------------
# DB init (cache store)
# ----------------------------
DB_PATH = st.secrets.get("DB_PATH", db_path_default())
init_db(DB_PATH)

# ----------------------------
# Header
# ----------------------------
st.title("ğŸ“ˆ æ±ºç®—çŸ­ä¿¡ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° & ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚º")
max_pdf_bytes = int(st.secrets.get("MAX_PDF_BYTES", 0) or 0)
if max_pdf_bytes > 0:
    st.caption(
        "ç‹™ã„ï¼šã‚¹ãƒãƒ›ã§ã‚‚ã€éŠ˜æŸ„â†’æ±ºç®—â†’è¦ç‚¹ï¼‹æ•°å€¤ã€ã¾ã§æœ€çŸ­ã§è¦‹ã‚‹ã€‚AIè¦ç´„ã¯æŠ¼ã—ãŸæ™‚ã ã‘å®Ÿè¡Œã€‚"
        f" / PDFä¸Šé™: {max_pdf_bytes/1024/1024:.1f}MB"
    )
else:
    st.caption("ç‹™ã„ï¼šã‚¹ãƒãƒ›ã§ã‚‚ã€éŠ˜æŸ„â†’æ±ºç®—â†’è¦ç‚¹ï¼‹æ•°å€¤ã€ã¾ã§æœ€çŸ­ã§è¦‹ã‚‹ã€‚AIè¦ç´„ã¯æŠ¼ã—ãŸæ™‚ã ã‘å®Ÿè¡Œã€‚")

# ----------------------------
# Screening controls
# ----------------------------
with st.expander("ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¡ä»¶", expanded=True):
    col1, col2, col3, col4 = st.columns([2, 2, 2, 2])

    with col1:
        code_in = st.text_input("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡ã€ç©ºãªã‚‰ç›´è¿‘å…¨ä½“ï¼‰", value="").strip()
        only_kessan = st.checkbox("æ±ºç®—çŸ­ä¿¡ã ã‘ã«çµã‚‹ï¼ˆ0ä»¶ãªã‚‰è‡ªå‹•ã§åºƒã‚ã«åˆ‡æ›¿ï¼‰", value=True)

    with col2:
        days = st.slider("ç›´è¿‘ä½•æ—¥ã‚’è¦‹ã‚‹ï¼Ÿ", 1, 30, 3)
        limit = st.slider("å–å¾—ä»¶æ•°ï¼ˆå¤§ãã„ã»ã©é…ã„ï¼‰", 50, 500, 200)

    with col3:
        only_has_doc_url = st.checkbox("PDF URLãŒã‚ã‚‹ã‚‚ã®ã ã‘", value=False)
        show_ai_button = st.checkbox("AIåˆ†æãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º", value=True)

    with col4:
        show_debug = st.checkbox("DEBUGè¡¨ç¤ºï¼ˆå…ˆé ­5ä»¶ã®JSONï¼‰", value=False)
        show_n = st.slider("ç”»é¢ã«è¡¨ç¤ºã™ã‚‹ä»¶æ•°", 20, 200, 100)

# sanity for code
code = ""
if code_in:
    if code_in.isdigit() and len(code_in) == 4:
        code = code_in
    else:
        st.warning("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã¯4æ¡ã®æ•°å­—ã§å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š7203ï¼‰")

# ----------------------------
# Fetch TDnet index (non-scrape) + cache
# ----------------------------
cutoff_utc = datetime.now(timezone.utc) - timedelta(days=days)


@st.cache_data(ttl=60, show_spinner=False)
def _cached_fetch_tdnet_items(code_: Optional[str], limit_: int) -> list[dict[str, Any]]:
    return fetch_tdnet_items(code_, limit=limit_)


with st.spinner("é–‹ç¤ºä¸€è¦§ã‚’å–å¾—ä¸­..."):
    items = _cached_fetch_tdnet_items(code or None, limit)

if show_debug:
    st.subheader("DEBUG: items å…ˆé ­5ä»¶ï¼ˆtitle/code/doc_url/link ã®æºã‚Œç¢ºèªï¼‰")
    st.json(items[:5])

# ----------------------------
# Normalize + Filter
# ----------------------------
normalized: list[dict[str, Any]] = []
for it in items:
    if not isinstance(it, dict):
        continue
    title, code_raw, doc_url, published_at = _extract_tdnet_fields(it)
    code4 = _code4(code_raw)

    normalized.append(
        {
            "title": title,
            "code": code4,
            "code_raw": code_raw,
            "doc_url": doc_url,
            "published_at": published_at,  # UTC
            "raw": it.get("raw") if isinstance(it.get("raw"), dict) else it,
        }
    )


def apply_filters(use_kessan: bool) -> list[dict[str, Any]]:
    out: list[dict[str, Any]] = []
    for it in normalized:
        title = it.get("title", "")
        doc_url = (it.get("doc_url") or "").strip()
        published = it.get("published_at")

        if use_kessan and not is_kessan(title):
            continue
        if only_has_doc_url and not doc_url:
            continue
        if isinstance(published, datetime) and published < cutoff_utc:
            continue
        out.append(it)
    return out


filtered = apply_filters(only_kessan)

# 0ä»¶ãªã‚‰è‡ªå‹•ã§åºƒã‚ã«ã™ã‚‹
if only_kessan and not filtered:
    st.info("ã€æ±ºç®—çŸ­ä¿¡ã ã‘ã€ã§0ä»¶ã ã£ãŸã®ã§ã€ãƒ•ã‚£ãƒ«ã‚¿ã‚’åºƒã’ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
    filtered = apply_filters(False)

st.subheader(f"å€™è£œï¼š{len(filtered)}ä»¶")
if not filtered:
    st.info("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹é–‹ç¤ºãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ—¥æ•°ã‚„ä»¶æ•°ã€ãƒ•ã‚£ãƒ«ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# AI availability
ai_ok = ai_is_enabled()
if show_ai_button and not ai_ok:
    st.warning("Gemini APIã‚­ãƒ¼æœªè¨­å®šã®ãŸã‚ã€AIåˆ†æã¯ç„¡åŠ¹ã§ã™ï¼ˆæ•°å€¤è¡¨ç¤ºã®ã¿ï¼‰ã€‚Secretsã« GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

# ----------------------------
# Render list
# ----------------------------
for i, it in enumerate(filtered[:show_n]):
    title = it.get("title", "")
    code_ = it.get("code", "") or "----"
    code_raw = it.get("code_raw", "") or ""
    doc_url = (it.get("doc_url") or "").strip()
    published = it.get("published_at")

    # è¡¨ç¤ºç”¨æ—¥æ™‚ï¼ˆUTCâ†’JSTï¼‰
    if isinstance(published, datetime):
        published_jst = published.astimezone(timezone(timedelta(hours=9)))
        published_str = published_jst.strftime("%Y-%m-%d %H:%M JST")
    else:
        published_str = "æ—¥æ™‚ä¸æ˜"

    # ä¸€æ„ã‚­ãƒ¼ï¼ˆURLãƒ™ãƒ¼ã‚¹ + indexï¼‰
    seed = f"{doc_url}|{published_str}|{title}|{i}"
    uid = hashlib.md5(seed.encode("utf-8")).hexdigest()[:12]

    label = f"{code_}({code_raw})ï½œ{published_str}ï½œ{title}"
    with st.expander(label, expanded=False):
        # --- PDFãƒªãƒ³ã‚¯ï¼ˆå®‰å…¨è¡¨ç¤ºï¼‰ ---
        _safe_pdf_link(doc_url)

        cached = get_cached_analysis(DB_PATH, doc_url) if doc_url else None
        if cached:
            st.success("è§£ææ¸ˆã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
            render_analysis(cached)
            st.caption("â€»åŒã˜PDF URLã¯SQLiteã«ä¿å­˜ã—ã€å†è§£æã—ã¾ã›ã‚“ï¼ˆDBã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ‰±ã„ï¼‰ã€‚")
            continue  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°ãƒœã‚¿ãƒ³ä¸è¦ï¼ˆé€Ÿã•å„ªå…ˆï¼‰

        st.info("æœªè§£æ")

        # AIåˆ†æå¯èƒ½æ¡ä»¶ï¼šAIæœ‰åŠ¹ + URLã‚ã‚Š + è¨±å¯ãƒ‰ãƒ¡ã‚¤ãƒ³
        allowed = bool(doc_url) and _is_allowed_pdf_url(doc_url)
        can_run_ai = show_ai_button and ai_ok and allowed

        if doc_url and not allowed:
            st.warning("å®‰å…¨ã®ãŸã‚ã€ã“ã®PDF URLã¯AIè§£æå¯¾è±¡å¤–ã§ã™ï¼ˆrelease.tdnet.info ã‚‚ã—ãã¯ yanoshin rd.php çµŒç”±ã®ã¿è¨±å¯ï¼‰ã€‚")

        run = st.button("AIåˆ†æ", key=f"ai_{uid}", disabled=not can_run_ai)

        if run:
            if not _check_pdf_size_or_warn(doc_url, max_pdf_bytes):
                st.stop()

            with st.spinner("AIãŒæ±ºç®—çŸ­ä¿¡ã‚’è§£æä¸­..."):
                try:
                    payload = analyze_pdf_to_json(doc_url)
                    save_analysis(DB_PATH, doc_url, code_, title, published, payload)
                    st.success("è§£æå®Œäº†")
                    render_analysis(payload)
                except Exception as e:
                    st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")

st.divider()

# ----------------------------
# Manual analyze
# ----------------------------
st.subheader("æ‰‹å‹•è§£æï¼ˆPDF URLã‚’è²¼ã‚‹ï¼‰")
manual = st.text_input("PDF URLï¼ˆrelease.tdnet.info ã® .pdf æ¨å¥¨ï¼‰", value="").strip()

colA, colB = st.columns([1, 3])
with colA:
    manual_allowed = _is_allowed_pdf_url(manual)
    manual_ok = ai_ok and manual_allowed
    manual_run = st.button("AIè§£æ", disabled=not manual_ok)

with colB:
    if manual and not manual_allowed:
        st.warning("å®‰å…¨ã®ãŸã‚ã€release.tdnet.info ã®PDFï¼ˆã¾ãŸã¯ yanoshin rd.php çµŒç”±ï¼‰ä»¥å¤–ã¯ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã„ã¾ã™ã€‚")
    else:
        st.caption("â€»AIæœ‰åŠ¹ï¼‹è¨±å¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã®PDF URLã®ã¿è§£æã—ã¾ã™ã€‚")

if manual:
    st.markdown("##### æ‰‹å‹•URLã®ãƒªãƒ³ã‚¯è¡¨ç¤º")
    _safe_pdf_link(manual)

if manual_run:
    if not _check_pdf_size_or_warn(manual, max_pdf_bytes):
        st.stop()

    cached = get_cached_analysis(DB_PATH, manual)
    if cached:
        st.success("è§£ææ¸ˆã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")
        render_analysis(cached)
    else:
        with st.spinner("AIãŒè§£æä¸­..."):
            try:
                payload = analyze_pdf_to_json(manual)
                # æ‰‹å‹•ã¯ã‚³ãƒ¼ãƒ‰ç­‰ãŒåˆ†ã‹ã‚‰ãªã„ã®ã§ç©ºã§ä¿å­˜ï¼ˆURLã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ååˆ†ï¼‰
                save_analysis(DB_PATH, manual, "", "manual", None, payload)
                st.success("è§£æå®Œäº†")
                render_analysis(payload)
            except Exception as e:
                st.error(f"è§£æã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
